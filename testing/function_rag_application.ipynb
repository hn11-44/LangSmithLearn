{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e87e6ec",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6b45c",
   "metadata": {},
   "source": [
    "Like all things we need to first have a proper setup, meaning we would need to setup our LangChain API key and our LLM model API key setup. Also that we have enable the correct tracing to the correct project, so that way we can monitor correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecaf2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "\n",
    "def _get_config(api_name: str):\n",
    "    if api_name in os.environ:\n",
    "        print(f\"Using {api_name} from environment\")\n",
    "        return os.environ[api_name]\n",
    "    else:\n",
    "        print(f\"{api_name} is not set in environments.\")\n",
    "        return getpass.getpass(f\"Please enter your {api_name} API Key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16337de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LANGCHAIN_API_KEY from environment\n",
      "Using GOOGLE_API_KEY from environment\n",
      "Using LANGCHAIN_TRACING_V2 from environment\n",
      "Using project: RAG Application Dan Koe\n",
      "Using USER_AGENT from environment\n"
     ]
    }
   ],
   "source": [
    "# Get LangSmith API Key \n",
    "LANGCHAIN_API_KEY = _get_config(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Get LLM Gemini \n",
    "GOOGLE_API_KEY = _get_config(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Set up tracing \n",
    "LANGCHAIN_TRACING_V2 = _get_config(\"LANGCHAIN_TRACING_V2\")\n",
    "\n",
    "# Set up a new project name\n",
    "os.environ['LANGCHAIN_PROJECT']=\"RAG Application Dan Koe\"\n",
    "print(f\"Using project: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "\n",
    "# Get User Agent for requests\n",
    "USER_AGENT = _get_config(\"USER_AGENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b14217",
   "metadata": {},
   "source": [
    "Now that we have set up our project we will be using different function in order to get to actually get some responses. We will be basing our RAG based on the Dan Koe's websites where he would actually posts his letters. So we wil start by automating the ingestion process where we would be relying on [Documents Loader](https://python.langchain.com/docs/integrations/document_loaders/) by relying on SitemapLoader to discover all the articles without blindly guessing URLs. In order to best trace these component we will be using function and relying on the `@traceable` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3aa1155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 43/43 [00:07<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded total of 43 documents\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nest_asyncio\n",
    "from langsmith import traceable \n",
    "from langchain_community.document_loaders import SitemapLoader\n",
    "from langchain_core.documents import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@traceable\n",
    "def load_documents_from_sitemap(sitemap_url: str, filtered_urls: list):\n",
    "    \"\"\"\n",
    "    A traceable function for discovering and loading all the articles from \n",
    "    a sitemap for specific posts URLS. \n",
    "\n",
    "    Args: \n",
    "    sitemap_url (str) : The URL of the sitemap to load documents from.\n",
    "    filtered_urls (list) : A list of specific post URLs to filter the documents.\n",
    "    \n",
    "    Returns: \n",
    "    list: A list of loaded document objects.\n",
    "    \"\"\"\n",
    "    # Sitemaper will automatically find and load all articles\n",
    "    loader = SitemapLoader(\n",
    "        web_path = sitemap_url, \n",
    "        filter_urls = filtered_urls\n",
    "    )\n",
    "\n",
    "    urls = [doc.metadata['source'] for doc in loader.load()]\n",
    "\n",
    "    loaded_documents = []\n",
    "\n",
    "    # Now, each URL neesds to be parsed\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Fetching Webpages from our Sitemap and URLS we got\n",
    "            response = requests.get(url, headers={\"User-Agent\": 'MyRAGBot/1.0'})\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parsing our URL Parsing with BeautifulSoup \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main_content = soup.select_one('article .body')\n",
    "\n",
    "            # We extract and store our clean text \n",
    "            if main_content: \n",
    "                text = main_content.get_text(separator=\"\\n\", strip = True)\n",
    "                loaded_documents.append(Document(page_content=text, metadata={\"source\": url}))\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    # Sanity Check - Verify the number of loaded documents\n",
    "    print(f\"Loaded total of {len(loaded_documents)} documents\")\n",
    "\n",
    "    return loaded_documents\n",
    "\n",
    "\n",
    "sitemap_url = \"https://letters.thedankoe.com/sitemap.xml\"\n",
    "fecthed_url = [\"https://letters.thedankoe.com/p\"]\n",
    "\n",
    "\n",
    "documents = load_documents_from_sitemap(sitemap_url, fecthed_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bfa23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the first 5 URLs found in the sitemap:\n",
      "This letter is long.\n",
      "This course was prev\n",
      "Most creators will f\n",
      "Before we begin, the\n",
      "Most people aren't b\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check \n",
    "if documents: # A better way to check if the list is not empty\n",
    "    print(\"\\nHere are the first 5 URLs found in the sitemap:\")\n",
    "    for i in range(min(5, len(documents))):\n",
    "        print(documents[i].page_content[:20])\n",
    "else: \n",
    "    print(\"\\nStill found 0 documents. There might be another problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75b75c",
   "metadata": {},
   "source": [
    "This means that we have fetched our documents, we have a total of 41 documents we are working with and just as a way to check that we have the correct content we have printed the the first 20 characters of our page content. So in the next step we would be chunking, this is nothing but creating a smaller content, more focused pieces of information than we require. \n",
    "\n",
    "We will reply on Text Splitter that would allow us to create turn the documents we have loaded into small yet precise retrieval that are large enough to contain complete and coherent thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82f5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Completed. Created a total of 773 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document \n",
    "\n",
    "\n",
    "@traceable \n",
    "def split_documents(documents : list[Document]) -> list[Document]:\n",
    "\n",
    "    \"\"\"\n",
    "    A @traceable decorated function that takes a list of documents and splits them into smaller, overlapping chunks. \n",
    "\n",
    "    Args: \n",
    "        documents (list[Document]): A list of Document objects to be split.\n",
    "\n",
    "    Returns: \n",
    "        list[Documents] : A list of Document objects representing the split chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Recursively Splits Long Text into smaller, semantically meaningful chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000, \n",
    "        chunk_overlap = 200, \n",
    "        add_start_index = True\n",
    "    )\n",
    "\n",
    "    # Split documents method works for chinking the documents \n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Sanity Check \n",
    "    print(f\"Splitting Completed. Created a total of {len(splits)} chunks.\")\n",
    "\n",
    "    return splits\n",
    "\n",
    "all_splits = split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585d4e5",
   "metadata": {},
   "source": [
    "Great! The documents have been split and now that we have 836 chunks we need to create an embedding (turning text-to-numbers) so that this way we are actually allowed to store it in a vector store. This will enable us to find text that is semantically close to the user's query. This is a two step process, where first we create an embedding model to convert it to meaningful numerical representation (vector) and the second step is a specialized database so that we can perform very fast similarity searches to the query vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70440f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created vector store.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma \n",
    "\n",
    "@traceable(\n",
    "        metadata={'vectordb': 'chroma'}, \n",
    "        run_type = \"retriever\"\n",
    "        )\n",
    "def build_retriver(chunks: list[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    A @traceable decorated function that creates a vector store from a list of documents.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[Document]): A list of Document objects to be added to the vector store.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: An instance of the Chroma vector store containing the document embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # This will be our transltor that will allow us to trun text into numbers \n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "    # We will use .from_documents class method to do the following: \n",
    "        # Take our lis of document chunks. \n",
    "        # Uses the provided embedding_model to turn them into vectors \n",
    "        # Creates a new Chroma database in memory and store documents \n",
    "    vector_store = Chroma.from_documents(documents=chunks, embedding = embedding_model)\n",
    "\n",
    "    print(\"Successfully created vector store.\")\n",
    "    return vector_store.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "\n",
    "retriever = build_retriver(all_splits)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95102dee",
   "metadata": {},
   "source": [
    "So to recap, we have successfully, ingested the documents from Dan Koe's letter from his website, then we have prepared the content of his posts into meaningful chunks then we have indexed the chunks by emedding them and using Chroma database so that we can effiicently retrieve responses based on the user's query. \n",
    "\n",
    "The next stage we will be putting all of thisngs together and we would actually build the RAG logic chain that will connect to the vector store to the LLM nad we will execute the chain by asking questions and getting responses. \n",
    "\n",
    "\n",
    "In the next stage we will further add more components that enable us to actually to retrieve the docuemnts that are relevant based on the user's question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "238587d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retrieve_documents(question: str, retriever: VectorStoreRetriever): \n",
    "\n",
    "    \"\"\"\n",
    "    A traceable function taking user's question and a retriever to return the most relevant\n",
    "    document chunk.\n",
    "\n",
    "    Args: \n",
    "        question (str): The user's question.\n",
    "        retriever: The retriever instance used to fetch relevant documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of the most relevant document chunks.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "    return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640a4c2",
   "metadata": {},
   "source": [
    "Now that we have set up our retriever we would need specify the format we would like our prompt to be returned. So we will define a function that will format the documents into a single string and then set up the final prompy which will be sent to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb119b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(run_type=\"prompt\")\n",
    "def format_documents(question: str, documents: list): \n",
    "\n",
    "    \"\"\"\n",
    "    Traceable function that format the retrieved documents and teh question into a structured prompt\n",
    "    so that it can be passed on to the LLM \n",
    "\n",
    "    Args: \n",
    "        question (str): The user's question \n",
    "        documents (list): The list of retrieved document chunks\n",
    "    \"\"\"\n",
    "    formatted_docs = \"\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "            Answer the question based on the following context.\n",
    "            If you don't know the answer disclose to the user that you do not know the answer.\n",
    "\n",
    "        Context:\n",
    "        {formatted_docs}\n",
    "\n",
    "        Question: \n",
    "        {question}\n",
    "         \"\"\"\n",
    "    \n",
    "    return final_prompt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdce3a",
   "metadata": {},
   "source": [
    "Now that we have set up the logic for the propt constriction we we would require our LLM to feed it into. We will be using our Gemini model and then we would also us a parser in order so as to only extract the content from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model_name = \"gemini-2.5-flash-lite\"\n",
    "temperature = 0.0\n",
    "@traceable(run_type = \"llm\", \n",
    "           metadata={'model_name': model_name, 'model_provider': 'google'})\n",
    "def call_llm(prompt: str, model :str, temperature:float = 0.0) -> str: \n",
    "\n",
    "    # Setup the LLM Model we are working with \n",
    "    model = ChatGoogleGenerativeAI(model = model_name, temperature = temperature)\n",
    "\n",
    "\n",
    "    chain = model | StrOutputParser()\n",
    "\n",
    "    return chain.invoke(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59404e09",
   "metadata": {},
   "source": [
    "Now we will bring everything together in our final rag chain where we would basically retriev the docuents and get the fomat for the prompt so that it can be inputted in to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c21c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(run_type=\"chain\")\n",
    "def rag_pipeline(question : str, retriever : VectorStoreRetriever): \n",
    "\n",
    "    \"\"\"\n",
    "    Orchasterates our entire RAG application by calling all of the specialist function, mainly\n",
    "    retrievr, formtted documents or promots that would be submitted into the LLM and the call to \n",
    "    the LLM as well. \n",
    "\n",
    "    Args:\n",
    "        question (str): The question to ask the LLM.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    retrieved_docs = retrieve_documents(question, retriever)\n",
    "    formatted_prompt = format_documents(question,retrieved_docs)\n",
    "    response = call_llm(formatted_prompt, model_name)\n",
    "\n",
    "    return response \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7bbfdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is the key to creating a successful one-person business, according to the author?\"\n",
    "final_answer = rag_pipeline(user_question, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e4da53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Answer ---\n",
      "The key to creating a successful one-person business, according to the author, is to **build a business around your life, not a life around your business**, prioritizing family and well-being. The author also emphasizes that **you are the niche** and that practicing **self-awareness** is the greatest business, marketing, and sales skill. Additionally, the author highlights the accessibility of the internet and writing as a powerful starting point for building a business, allowing individuals to become \"one-person media companies.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final Answer ---\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "829bcab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      "The author suggests that to set someone up for life, one should focus on developing \"liberating arts\" rather than just career-specific skills. These liberating arts are:\n",
      "\n",
      "*   **Logic:** How to derive truth from known facts.\n",
      "*   **Statistics:** How to understand the implications of data.\n",
      "*   **Rhetoric:** How to persuade and spot persuasion tactics.\n",
      "*   **Research:** How to gather information on an unknown subject.\n",
      "*   **(Practical) Psychology:** How to discern and understand the true motives of others.\n",
      "*   **Investment:** How to manage and grow existing assets.\n",
      "*   **Self-development:** Cultivating a valuable mindset and skillset that can help others expand beyond their limits.\n",
      "*   **Self-reliance:** How to get what you want by taking responsibility for the outcome of your life.\n",
      "*   **Self-education:** The ability to gather, make sense of, and utilize information on an unknown subject.\n",
      "*   **Self-sufficiency:** The ability to sustain oneâ€™s ideal lifestyle and acquire the resources necessary to do so.\n",
      "*   **Self-mastery:** An unwavering dedication to the process of navigating reality.\n",
      "*   **Agency:** How to make decisions about what course to pursue, and proactively take action to pursue it.\n",
      "*   **Risk tolerance:** The ability to embrace uncertainty, as wealth creation often lies in this.\n",
      "\n",
      "The author emphasizes that these skills are learned through the pursuit of one's interests, through trial and error toward a self-generated goal, and by learning how to learn, think, live, and earn.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What are the author suggest in terms of skills that would set someone up for life\"\n",
    "answer = rag_pipeline(user_question, retriever)\n",
    "\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
