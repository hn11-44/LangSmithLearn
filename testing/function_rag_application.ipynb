{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e87e6ec",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6b45c",
   "metadata": {},
   "source": [
    "Like all things we need to first have a proper setup, meaning we would need to setup our LangChain API key and our LLM model API key setup. Also that we have enable the correct tracing to the correct project, so that way we can monitor correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecaf2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "\n",
    "def _get_config(api_name: str):\n",
    "    if api_name in os.environ:\n",
    "        print(f\"Using {api_name} from environment\")\n",
    "        return os.environ[api_name]\n",
    "    else:\n",
    "        print(f\"{api_name} is not set in environments.\")\n",
    "        return getpass.getpass(f\"Please enter your {api_name} API Key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16337de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LANGCHAIN_API_KEY from environment\n",
      "Using GOOGLE_API_KEY from environment\n",
      "Using LANGCHAIN_TRACING_V2 from environment\n",
      "Using project: RAG Application Dan Koe\n",
      "Using USER_AGENT from environment\n"
     ]
    }
   ],
   "source": [
    "# Get LangSmith API Key \n",
    "LANGCHAIN_API_KEY = _get_config(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "# Get LLM Gemini \n",
    "GOOGLE_API_KEY = _get_config(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Set up tracing \n",
    "LANGCHAIN_TRACING_V2 = _get_config(\"LANGCHAIN_TRACING_V2\")\n",
    "\n",
    "# Set up a new project name\n",
    "os.environ['LANGCHAIN_PROJECT']=\"RAG Application Dan Koe\"\n",
    "print(f\"Using project: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "\n",
    "# Get User Agent for requests\n",
    "USER_AGENT = _get_config(\"USER_AGENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b14217",
   "metadata": {},
   "source": [
    "Now that we have set up our project we will be using different function in order to get to actually get some responses. We will be basing our RAG based on the Dan Koe's websites where he would actually posts his letters. So we wil start by automating the ingestion process where we would be relying on [Documents Loader](https://python.langchain.com/docs/integrations/document_loaders/) by relying on SitemapLoader to discover all the articles without blindly guessing URLs. In order to best trace these component we will be using function and relying on the `@traceable` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3aa1155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 42/42 [00:08<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded total of 42 documents\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import nest_asyncio\n",
    "from langsmith import traceable \n",
    "from langchain_community.document_loaders import SitemapLoader\n",
    "from langchain_core.documents import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@traceable\n",
    "def load_documents_from_sitemap(sitemap_url: str, filtered_urls: list):\n",
    "\n",
    "    \"\"\"\n",
    "    A traceable function for discovering and loading all the articles from \n",
    "    a sitemap for specific posts URLS. \n",
    "\n",
    "    Args: \n",
    "    sitemap_url (str) : The URL of the sitemap to load documents from.\n",
    "    filtered_urls (list) : A list of specific post URLs to filter the documents.\n",
    "    \n",
    "    Returns: \n",
    "    list: A list of loaded document objects.\n",
    "    \"\"\"\n",
    "    # Sitemaper will automatically find and load all articles\n",
    "    loader = SitemapLoader(\n",
    "        web_path = sitemap_url, \n",
    "        filter_urls = filtered_urls\n",
    "    )\n",
    "\n",
    "    urls = [doc.metadata['source'] for doc in loader.load()]\n",
    "\n",
    "    loaded_documents = []\n",
    "\n",
    "    # Now, each URL neesds to be parsed\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Fetching Webpages from our Sitemap and URLS we got\n",
    "            response = requests.get(url, headers={\"User-Agent\": 'MyRAGBot/1.0'})\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parsing our URL Parsing with BeautifulSoup \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            main_content = soup.select_one('article .body')\n",
    "\n",
    "            # We extract and store our clean text \n",
    "            if main_content: \n",
    "                text = main_content.get_text(separator=\"\\n\", strip = True)\n",
    "                loaded_documents.append(Document(page_content=text, metadata={\"source\": url}))\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "\n",
    "    # Sanity Check - Verify the number of loaded documents\n",
    "    print(f\"Loaded total of {len(loaded_documents)} documents\")\n",
    "\n",
    "    return loaded_documents\n",
    "\n",
    "\n",
    "sitemap_url = \"https://letters.thedankoe.com/sitemap.xml\"\n",
    "fecthed_url = [\"https://letters.thedankoe.com/p\"]\n",
    "\n",
    "\n",
    "documents = load_documents_from_sitemap(sitemap_url, fecthed_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53bfa23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are the first 5 URLs found in the sitemap:\n",
      "This course was prev\n",
      "Most creators will f\n",
      "Before we begin, the\n",
      "Most people aren't b\n",
      "This post builds on \n"
     ]
    }
   ],
   "source": [
    "# Sanity Check \n",
    "if documents: # A better way to check if the list is not empty\n",
    "    print(\"\\nHere are the first 5 URLs found in the sitemap:\")\n",
    "    for i in range(min(5, len(documents))):\n",
    "        print(documents[i].page_content[:20])\n",
    "else: \n",
    "    print(\"\\nStill found 0 documents. There might be another problem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75b75c",
   "metadata": {},
   "source": [
    "This means that we have fetched our documents, we have a total of 41 documents we are working with and just as a way to check that we have the correct links we have printed the the first five URLS. So in the next step we would be chunking, this is nothing but creating a smaller content, more focused pieces of information than we require. \n",
    "\n",
    "We will reply on Text Splitter that would allow us to create turn the documents we have loaded into small yet precise retrieval that are large enough to contain complete and coherent thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82f5585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Completed. Created a total of 735 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document \n",
    "\n",
    "\n",
    "@traceable \n",
    "def split_documents(documents : list[Document]) -> list[Document]:\n",
    "\n",
    "    \"\"\"\n",
    "    A @traceable decorated function that takes a list of documents and splits them into smaller, overlapping chunks. \n",
    "\n",
    "    Args: \n",
    "        documents (list[Document]): A list of Document objects to be split.\n",
    "\n",
    "    Returns: \n",
    "        list[Documents] : A list of Document objects representing the split chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Recursively Splits Long Text into smaller, semantically meaningful chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000, \n",
    "        chunk_overlap = 200, \n",
    "        add_start_index = True\n",
    "    )\n",
    "\n",
    "    # Split documents method works for chinking the documents \n",
    "    splits = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Sanity Check \n",
    "    print(f\"Splitting Completed. Created a total of {len(splits)} chunks.\")\n",
    "\n",
    "    return splits\n",
    "\n",
    "all_splits = split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585d4e5",
   "metadata": {},
   "source": [
    "Great! The documents have been split and now that we have 836 chunks we need to create an embedding (turning text-to-numbers) so that this way we are actually allowed to store it in a vector store. This will enable us to find text that is semantically close to the user's query. This is a two step process, where first we create an embedding model to convert it to meaningful numerical representation (vector) and the second step is a specialized database so that we can perform very fast similarity searches to the query vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70440f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created vector store.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma \n",
    "\n",
    "@traceable\n",
    "def create_vector_store(chunks: list[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    A @traceable decorated function that creates a vector store from a list of documents.\n",
    "\n",
    "    Args:\n",
    "        chunks (list[Document]): A list of Document objects to be added to the vector store.\n",
    "\n",
    "    Returns:\n",
    "        Chroma: An instance of the Chroma vector store containing the document embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # This will be our transltor that will allow us to trun text into numbers \n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "    # We will use .from_documents class method to do the following: \n",
    "        # Take our lis of document chunks. \n",
    "        # Uses the provided embedding_model to turn them into vectors \n",
    "        # Creates a new Chroma database in memory and store documents \n",
    "    vector_store = Chroma.from_documents(documents=chunks, embedding = embedding_model)\n",
    "\n",
    "    print(\"Successfully created vector store.\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "vector_store = create_vector_store(all_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95102dee",
   "metadata": {},
   "source": [
    "So to recap, we have successfully, ingested the documents from Dan Koe's letter from his website, then we have prepared the content of his posts into meaningful chunks then we have indexed the chunks by emedding them and using Chroma database so that we can effiicently retrieve responses based on the user's query. \n",
    "\n",
    "The next stage we will be putting all of thisngs together and we would actually build the RAG logic chain that will connect to the vector store to the LLM nad we will execute the chain by asking questions and getting responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b01a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "@traceable \n",
    "def setup_rag_chain(vector_store):\n",
    "\n",
    "    \"\"\"\n",
    "    Setup up and returns tracebale RAG chain. \n",
    "\n",
    "    Args: \n",
    "        vector_store (Chroma): The vector store to use for the RAG chain.\n",
    "\n",
    "    Returns:\n",
    "        Chaining: The constructed RAG chain.\n",
    "    \"\"\"\n",
    "    # Setup the LLM Model we are working with \n",
    "    model = ChatGoogleGenerativeAI(model ='gemini-2.5-flash-lite', temperature = 1)\n",
    "\n",
    "    # Setup the retriver \n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "    # Setup the prompt template\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Answer the question based on the following context.\n",
    "        If you don't know the answer disclose to the user that you do not know the answer.\n",
    "        \n",
    "        Context: \n",
    "        {context}\n",
    "\n",
    "        Question: \n",
    "        {question}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    def format_docs(docs): \n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt_template\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734ea44",
   "metadata": {},
   "source": [
    "To follow what has been done here, we are basically defining data transformations that are taking place at each stage. So our `rag_chain`basically would take the question and pass it to the retriever and also simply takes the original question and passess it through unchanged. Then we would be taking the dictionary of the context and the questions and pipe it to the prompt, where LangChain would automatically match the keys of dictionary context and question to their respective placeholders. Then the fomatted prompted is piped into the Gemini model to generate the answer and the output is parsed to just pull out the content giving us a clean string at as the final answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f835e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build you chain using vector_store\n",
    "rag_chain = setup_rag_chain(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c751f2",
   "metadata": {},
   "source": [
    "So now we will create a new function that would allow us to ask questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c34aaff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable \n",
    "def ask_question(chain, question: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Invoke the RAG chain with a question retruning the answer. \n",
    "    \"\"\"\n",
    "    return chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98010c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25ef093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      "The author suggests reinventing yourself by stepping into the unknown, which is described as being \"reborn.\" This process involves a buffer period of high anxiety as nature tests your seriousness about your capabilities. The author outlines two steps for engineering an identity:\n",
      "\n",
      "1.  **Recognition:** Understand that you are pursuing goals, whether consciously or not. These goals shape your interpretation of reality. Recognize that your current goals likely stem from conditioning by your environment (parents, teachers, culture), which may be leading you to a dead end.\n",
      "2.  **Strategic Dissonance:** Actively widen the gap between who you are and who you want to be. Cultivate dissatisfaction with your current lifestyle by considering where you will end up if you don't change.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How does the author suggest someone reinvent themselves?\"\n",
    "answer = ask_question(rag_chain, user_question)\n",
    "\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d87c989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      "The author suggests that to be set up for life, one should focus on learning the \"liberating arts\" which are:\n",
      "\n",
      "*   **Logic**: how to derive truth from known facts\n",
      "*   **Statistics**: how to understand the implications of data\n",
      "*   **Rhetoric**: how to persuade, and spot persuasion tactics\n",
      "*   **Research**: how to gather information on an unknown subject\n",
      "*   **(Practical) Psychology**: how to discern and understand the true motives of others\n",
      "*   **Investment**: how to manage and grow existing assets\n",
      "*   **Agency**: how to make decisions about what course to pursue, and proactively take action to pursue it\n",
      "*   **Risk Tolerance**: the ability to embrace uncertainty\n",
      "\n",
      "In addition to these, the author also emphasizes the importance of self-development, self-reliance, self-education, self-sufficiency, and self-mastery. The author believes these are critical for creating value and navigating reality.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"What are the author suggest in terms of skills that would set someone up for life\"\n",
    "answer = ask_question(rag_chain, user_question)\n",
    "\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac1ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
